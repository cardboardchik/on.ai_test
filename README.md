
# REST API-сервис для обработки webhook-запросов с использованием LLM

## Описание проекта
Данный проект представляет собой REST API-сервис, разработанный для обработки входящих webhook-запросов. Сервис использует LLM (GPT-3.5-turbo) для генерации ответов и отправляет результаты на указанный callback URL. Проект поддерживает сохранение контекста диалога и имеет базовые функции логирования, валидации и обработки ошибок.
Можно было сделать структуру получше, но я решил, что так будет удобнее) а то гулять по папочкам в таком маленьком проекте так себе :)

## Функциональные возможности
- **POST /webhook**: Принимает входящие запросы с сообщением и callback URL.
- **LLM интеграция**: Подключение к LLM для генерации ответов.
- **Отправка результатов**: Автоматическая отправка сгенерированных ответов на указанный callback URL.
- **Поддержка истории**: Сохранение контекста диалога для поддержания последовательности общения (id - это наш callback URL).
- **Асинхронная обработка**: Использование Celery для обработки запросов в фоновом режиме.

## Требования
- **Python** 3.8 или выше
- **Docker** и **Docker Compose** (рекомендуется)
- **Redis** для работы с Celery

## Установка и запуск

### Запуск с использованием Docker Compose
1. Клонируйте репозиторий.
2. Скопируйте `.env-sample` в `.env` и настройте необходимые переменные окружения.
3. Запустите сборку и приложение с помощью команды:
   ```bash
   docker-compose up --build
   ```

### Локальный запуск без Docker
1. Создайте и активируйте виртуальное окружение:
   ```bash
   python -m venv venv
   source venv/bin/activate  # В Windows: `venv\Scripts\activate`
   ```
2. Установите зависимости:
   ```bash
   pip install -r requirements.txt
   ```
3. Запустите FastAPI приложение:
   ```bash
   uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
   ```
4. Запустите Celery worker:
   ```bash
   celery -A app.worker worker --loglevel=info
   ```

## Использование API

### Endpoint: POST /webhook
**Описание**: Принимает входящие сообщения и обрабатывает их с использованием LLM. Затем возвращает ответ на callback_url.
 
**Тело запроса**:
```json
{
    "message": "текст запроса",
    "callback_url": "https://web:8000/callback"
}
```

**Ответ**:
- `200 OK`: Успешно принят запрос и обработан.
```json
{
  "task_id": "fff755e1-5131-4bec-b76c-537e0b10b6e6",
  "status": "Processing"
}
```
- `429 Too Many Requests`: больше пяти запросов в минуту.
- `422 UNPROCESSABLE ENTITY`: Ошибка валидации входных данных.
```json
{
  "detail": [
    {
      "type": "url_parsing",
      "loc": [
        "body",
        "callback_url"
      ],
      "msg": "Input should be a valid URL, relative URL without a base",
      "input": "hйцуttp://web:8000/callback",
      "ctx": {
        "error": "relative URL without a base"
      }
    }
  ],
  "body": {
    "message": "text",
    "callback_url": "hйцуttp://web:8000/callback"
  }
}
```

### Endpoint: POST /callback
**Описание**: Просто для проверки.
 
**Тело запроса**:

```json
{
  "generated_response": "text"
}
```

**Ответ**:
- `200 OK`: Успешно принят запрос и обработан.
- `429 Too Many Requests`: больше пяти запросов в минуту.

## Переменные окружения
Настройте следующие переменные в файле `.env`:
- `API_KEY`: Для Openrouter.ai(openai/gpt-3.5-turbo-1106).


## Логирование и обработка ошибок
Проект поддерживает базовое логирование операций и обработку ошибок, что позволяет отслеживать и управлять исключениями.

## Покрытие базовыми тестами

## Документация API
Документация автоматически генерируется и доступна по следующим адресам:
- **Swagger UI**: `http://localhost:8000/docs`
- **ReDoc**: `http://localhost:8000/redoc`

